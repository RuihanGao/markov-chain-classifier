{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "from torch.utils import data as data2\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data2.Dataset):\n",
    "    def __init__(self, list_IDs, labels):\n",
    "        # initialize\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = torch.load('slide_6_10/' + ID + '.pt')\n",
    "        X.unsqueeze_(0)\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN models\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(CNN, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3,5))\n",
    "        self.conv1_drop = nn.Dropout2d(p=0.8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('Conv:', x.size())\n",
    "        x = self.conv1(x)\n",
    "        # print('Conv', x.size())\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        # print('Pool', x.size())\n",
    "        x = x.view(-1, 3*2*3)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN(seq_len)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=3*2*3, \n",
    "            hidden_size=50, \n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "           dropout=0.8)\n",
    "        \n",
    "        self.linear = nn.Linear(50,23)\n",
    "        self.hidden = []\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, h, c):\n",
    "        self.hidden = (h, c)\n",
    "        # Set initial hidden and cell states: initialize outside \n",
    "        #return (h, c) # (torch.zeros(2, batch_size, 50).to(device) , torch.zeros(2, batch_size, 50).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(x.size())\n",
    "        batch_size, timesteps, C, H, W, sequence_size = x.size()\n",
    "        #print(batch_size*timesteps,C, H, W, sequence_size)\n",
    "        c_in = x.view(batch_size * timesteps*sequence_size, C, H, W)\n",
    "        #print(c_in.size())\n",
    "        \n",
    "        c_out = self.cnn(c_in)\n",
    "        #print(c_out.size())\n",
    "        \n",
    "        r_in = c_out.view(batch_size,sequence_size,-1)\n",
    "        r_out, (h_n, h_c) = self.lstm( r_in, self.hidden)#(self.hidden[0][:,:batch_size,:], self.hidden[1][:,:batch_size,:] ))\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "\n",
    "        return F.log_softmax(r_out2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=23\n",
    "num_epochs = 3000\n",
    "learning_rate = 0.001\n",
    "log_interval = 10\n",
    "save_interval  = 200\n",
    "seq_len = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_learn(iteration):\n",
    "    \n",
    "    dirName = 'slide_info_' + str(iteration)\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(dirName)\n",
    "    except FileExistsError:\n",
    "        print('Re-writing the touch')\n",
    "    dirName = dirName + '/'\n",
    "    \n",
    "    # load data\n",
    "    [train_ids, train_labels, test_ids, test_labels] = pickle.load(open('slide_6_10.pkl', 'rb'))\n",
    "    training_dataset = Dataset(train_ids, train_labels)\n",
    "    test_dataset = Dataset(test_ids, test_labels)\n",
    "    train_loader = data2.DataLoader(training_dataset, batch_size=batch_size)\n",
    "    test_loader = data2.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    model = CNN_LSTM(seq_len).to(device)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # set initial hidden state\n",
    "    (h_ini, c_ini) = (torch.zeros(2, batch_size, 50).to(device) , torch.zeros(2, batch_size, 50).to(device))\n",
    "    \n",
    "    epoch_lists = []\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data = np.expand_dims(data, axis=1)\n",
    "            data = torch.FloatTensor(data)\n",
    "            #print(target.size())\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            #print('Data size:', data.size())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # init hidden states\n",
    "            model.init_hidden(h_ini, c_ini)\n",
    "            \n",
    "            output = model(data)\n",
    "            #print(target.size(), output.size())\n",
    "\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if batch_idx % log_interval == 0:\n",
    "        epoch_lists.append(loss.item())\n",
    "        if epoch % save_interval == 0:\n",
    "            torch.save(model.state_dict(), dirName + 'model_epoch_' + str(epoch) +'.ckpt')\n",
    "         \n",
    "    # save epochs\n",
    "    pickle.dump(epoch_lists, open(dirName + 'loss.pkl', 'wb'))\n",
    "    \n",
    "    ## check for accuracy\n",
    "        \n",
    "    results = []\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in train_loader:\n",
    "\n",
    "        data = np.expand_dims(data, axis=1)\n",
    "        data = torch.FloatTensor(data)        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(\n",
    "            output, target).data[0]  # sum up batch loss\n",
    "        pred = output.data.max(\n",
    "            1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(train_loader.dataset)\n",
    "    results.append( 100.0 * correct / len(train_loader.dataset) )\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = np.expand_dims(data, axis=1)\n",
    "        data = torch.FloatTensor(data)        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(\n",
    "            output, target).data[0]  # sum up batch loss\n",
    "        pred = output.data.max(\n",
    "            1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    results.append( 100.0 * correct / len(test_loader.dataset) )\n",
    "    \n",
    "    pickle.dump(results, open(dirName + 'results.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
